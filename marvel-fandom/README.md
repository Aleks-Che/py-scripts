# Marvel Fandom Parser

Набор скриптов для парсинга сайта marvel.fandom.com

## Состав

1. **parser.py** - Скрипт для получения списков ссылок на страницы персонажей
2. **content_parser.py** - Скрипт для парсинга содержимого страниц

## Установка зависимостей

```bash
pip install requests beautifulsoup4 selenium webdriver-manager urllib3
```

Для работы скрипта `content_parser.py` также потребуется:

- Chrome браузер
- ChromeDriver (автоматически устанавливается через webdriver-manager)

## Использование

### 1. Получение списков ссылок

```bash
python parser.py --page-url "URL_КАТЕГОРИИ" -o output.json
```

Пример:

```bash
python parser.py --page-url "https://marvel.fandom.com/ru/wiki/Категория:Суперзлодеи" -o marvel-fandom/output/supervillains.json
```

### 2. Парсинг содержимого страниц

Скрипт `content_parser.py` автоматически обрабатывает все JSON файлы в каталоге `marvel-fandom/output/` и сохраняет результаты в `marvel-fandom/parsed_content/`.

```bash
python content_parser.py
```

## Настройка констант

В `content_parser.py` можно изменить:

- `INPUT_DIR` - каталог с JSON файлами со списками (по умолчанию: `marvel-fandom/output`)
- `OUTPUT_DIR` - выходной каталог (по умолчанию: `marvel-fandom/parsed_content`)
- `MAIN_CONTENT_SELECTOR` - CSS селектор для основного контента
- `SHORT_PAUSE_MIN/MAX` - минимальная/максимальная пауза между запросами (секунды)
- `LONG_PAUSE_MIN/MAX` - минимальная/максимальная длинная пауза (секунды)
- `PAGES_BEFORE_LONG_PAUSE` - количество страниц перед длинной паузой

## Структура выходных данных

Для каждого JSON файла из входного каталога создается подкаталог в выходном каталоге. В каждом подкаталоге сохраняются отдельные JSON файлы для каждой страницы.

Формат выходного JSON:

```json
{
  "title": "Название персонажа",
  "url": "URL страницы",
  "main_content": "Основной текст статьи",
  "sections": [
    {
      "level": 2,
      "title": "Название секции",
      "content": "Содержимое секции"
    }
  ],
  "infobox": {
    "name": "Имя персонажа",
    "Личность": {
      "Позиция": "Нейтралитет",
      "Секрет личности": "Нет второй личности"
    }
  },
  "categories": ["Категория1", "Категория2"],
  "raw_html": "HTML код основного контента"
}
```

## Особенности

- Используется Selenium WebDriver для обхода защиты сайта
- Автоматические паузы между запросами для избежания блокировки
- Обработка ошибок с сохранением информации о проблемах
- Пропуск уже обработанных страниц
- Поддержка русского языка (UTF-8)

## Рекомендации

1. **Паузы**: Скрипт автоматически делает паузы между запросами, но при большом объеме данных рекомендуется использовать прокси или VPN
2. **Повторный запуск**: Скрипт можно безопасно перезапускать - он пропустит уже обработанные страницы и полностью обработанные файлы
3. **Ошибки**: Если страница не загружена, будет создан файл с описанием ошибки
4. **Производительность**: Для больших объемов данных рекомендуется запускать скрипт на ночь
5. **Проверка завершенности**: Скрипт автоматически проверяет, обработан ли весь файл, и пропускает его если все страницы уже спарсены

## Проблемы и решения

### Ошибка "selenium не установлен"

```bash
pip install selenium webdriver-manager
```

### ChromeDriver не найден

Убедитесь, что установлен Chrome браузер. WebDriver Manager автоматически скачает нужную версию ChromeDriver.

### Сайт блокирует запросы

- Используйте VPN или прокси
- Увеличьте паузы между запросами в коде
- Попробуйте запустить скрипт в другое время

### Не находит основной контент

Проверьте селектор `MAIN_CONTENT_SELECTOR` и при необходимости обновите его в соответствии с текущей структурой сайта.
